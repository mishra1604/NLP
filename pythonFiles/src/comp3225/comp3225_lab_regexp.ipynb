{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions Lab\n",
    "\n",
    "Les Carr, Last update 19/01/2023\n",
    "\n",
    "Copyright University of Southampton, 2021.\n",
    "Permission is granted for copies to be made for personal use by University of Southampton students. This content should not be shared on published outside the University of Southampton.\n",
    "\n",
    "Regular expressions (aka regexes or regexps) are text matching patterns described with a formal syntax that were covered in Lecture 3. This lab covers two main issues\n",
    "- Basic Python Capabilities for Using Regular Expressions\n",
    "- How to use regular expressions for a variety of tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Python Capabilities for Regular Expressions\n",
    "\n",
    "Regular expressions are implemented in the `re` package, which provides a number of functions to match regexps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'NLP' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/aarya/Documents/Southampton/COMPSCI/Year 3/Natural Language Processing/NLP/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# List of patterns to search for\n",
    "\n",
    "# Text to parse\n",
    "text = 'This is a string with term1.'\n",
    "if re.search(\"term[0-4]\",  text):\n",
    "        print('first term.')\n",
    "if re.search(\"term[5-9]\",  text):\n",
    "        print ('Found second term.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've seen that `re.search()` will take the pattern, scan the text, and then returns a **Match** object corresponding to the first match. If no pattern is found, a **None** is returned. Both of these are truthy, so any successful match (even a match on a null string from an ill-advised Kleene \\*) will evaluate to `True` in a `Boolean` context. To give a clearer picture of this match object, check out the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '3225' between positions 16 and 20\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"\\d+\",  \"This is the COMP3225 module's 1st lab\")\n",
    "\n",
    "print(\"The regexp matched '%s' between positions %s and %s\" % (match.group(0), match.start(), match.end()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you used capture groups in the regular expression, they will appear as arguments `1` up to `99` of the `match.groups()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '3225' and '2'\n"
     ]
    }
   ],
   "source": [
    "match = re.search(\"(\\d+).*(\\d+)\",  \"This is the COMP3225 module's 12st lab\")\n",
    "\n",
    "print(\"The regexp matched '%s' and '%s'\" % (match.group(1), match.group(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variants of the search function:\n",
    "- `re.match()` is anchored at the beginning of the search string\n",
    "- `re.fullmatch()` is anchored at the beginning and the end of the search string\n",
    "- `re.findall()` return all matches \n",
    "\n",
    "You can also look for all the matches in a string with `re.findall()`, but it returns a list of the actual strings matched rather than a `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regexp matched '['3225', '1', '2']'\n"
     ]
    }
   ],
   "source": [
    "match = re.findall(\"\\d+\",  \"This is the COMP3225 module's 1st lab with 2nd lab\")\n",
    "\n",
    "print(\"The regexp matched '%s'\" % (match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed this in a _file-read-and-match-print-results_ code fragment that works line by line, we can do the following. We are using the codecs package to explicitly manage the various character sets that we might encounter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2021', '02380594479']\n",
      "['19865']\n",
      "['20', '49', '9']\n",
      "['0', '10']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "fname=\"../../corpus/comp3225/mytest.txt\"\n",
    "\n",
    "for line in codecs.open(fname,\"r\",encoding=\"utf-8\"):\n",
    "    match=re.findall(\"\\d+\", line)\n",
    "    if match: print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there are four usefull flags you can use to turn on different features in Python's implementation of regular expressions. \n",
    "- VERBOSE. Allow inline comments and extra whitespace.\n",
    "- IGNORECASE. Do case-insensitive matches.\n",
    "- DOTALL. Allow dot (.) to match any character, including a newline. (The default behavior of dot is to match anything, except for a newline.)\n",
    "- MULTILINE. Allow anchors (^ and $) to match the beginnings and ends of lines instead of matching the beginning and end of the whole text.\n",
    "\n",
    "They can be provided as flags to the re methods\n",
    "- re.match(\"this\", \"This\", flags=re.IGNORECASE)\n",
    "\n",
    "or as abbreviated flags\n",
    "- re.match(\"that\", \"That\", flags=re.I)\n",
    "\n",
    "or as inline flags in the regular expression itself\n",
    "- re.match(\"(?i)those\", \"Those\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing Regular Expressions\n",
    "\n",
    "You should now have a good enough understanding of how to use the regular expressions module in Python to undertake the rest of the tasks in the lab. You should refer to the full [Python regular expression documentation](https://docs.python.org/3/library/re.html)  for more detail.\n",
    "\n",
    "Note that so far we haven't needed to use multiline or raw Python strings, or verbose regular expressions. As things get more complex, you might need to.\n",
    "\n",
    "The following six tasks consist of three skills tasks and three application tasks. You should attempt all the skills tasks (1-3) to your satisfaction, and choose any of the application tasks (4-6) for general practice in using regular expressions to solve problems that require complex lexical analysis.\n",
    "\n",
    "Please note: there are no right answers set for these tasks - no specific precision or recall targets that you have to meet. The intention is that you focus on practicing the use of regular expressions, and gain experience with applying different kinds of \"patterns\" to a range of situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - recognising specific lexical tokens\n",
    "Recognise the following particular kinds of non-word lexical tokens, using your experience to define the allowable format of each token type. A whole load of data from the internet (including some HTML code) has been put into an example test file for you.\n",
    "\n",
    "1. Hashtags (which can contain alphanumeric characters, underscores and hyphens). Test on the text of some tweets that you copy and paste from twitter.com.\n",
    "1. UK Postcodes like SO17 1BJ or E1 8BN.\n",
    "1. UK style phone numbers like 023 80591234 or 02380 590000 or +44 (0) 23 80594479 or +44 (23) 8059 5000 or mobile numbers like 07722 175921.\n",
    "1. URLs http://google.com/ or https://a.b.net/c/d/e.php#fragment.\n",
    "1. Email addresses like lac@ecs.soton.ac.uk, and decompose into username and internet domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 30), match=': Bedfordshire, England'>\n",
      "<re.Match object; span=(26, 41), match='y Park, Bedford'>\n",
      "<re.Match object; span=(7, 27), match=': Yorkshire, England'>\n",
      "<re.Match object; span=(13, 30), match='y Walk, Kirkstall'>\n",
      "<re.Match object; span=(7, 32), match=': Leicestershire, England'>\n",
      "<re.Match object; span=(19, 36), match='n Road, Leicester'>\n",
      "<re.Match object; span=(13, 30), match='t Gallery, Kendal'>\n",
      "<re.Match object; span=(7, 25), match=': Cumbria, England'>\n",
      "<re.Match object; span=(18, 38), match='t Scotland, Scotland'>\n",
      "<re.Match object; span=(7, 26), match=': Shiprow, Aberdeen'>\n",
      "<re.Match object; span=(7, 29), match=': Oxfordshire, England'>\n",
      "<re.Match object; span=(14, 31), match='t Place, Abingdon'>\n",
      "<re.Match object; span=(7, 28), match=': Shropshire, England'>\n",
      "<re.Match object; span=(7, 25), match=': Suffolk, England'>\n",
      "<re.Match object; span=(16, 33), match='t Hall, Aldeburgh'>\n",
      "<re.Match object; span=(35, 49), match='n Park, Newton'>\n",
      "<re.Match object; span=(40, 55), match='n House, Queens'>\n",
      "<re.Match object; span=(63, 78), match='y House, Hunter'>\n",
      "['+44 (23) 8059 5000']\n",
      "['SO22 4NR', 'SO22 4NR']\n",
      "<re.Match object; span=(40, 53), match='y Park, Burnt'>\n",
      "<re.Match object; span=(43, 60), match='n House, Chorlton'>\n",
      "<re.Match object; span=(54, 66), match='y House, The'>\n",
      "<re.Match object; span=(40, 53), match='n Campus, The'>\n",
      "<re.Match object; span=(17, 39), match='n Birkbeck, University'>\n",
      "<re.Match object; span=(15, 33), match='n City, University'>\n",
      "<re.Match object; span=(60, 78), match='r Place, Belgravia'>\n",
      "<re.Match object; span=(30, 54), match='n University, Birmingham'>\n",
      "<re.Match object; span=(58, 73), match='y House, Gibbet'>\n",
      "['#covid']\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "fname=\"../../corpus/comp3225/misctext.txt\"\n",
    "hashtag_re=\"#covid\"\n",
    "postcode_re=\"SO22 4NR\"\n",
    "phone_re=\"\\+44 \\(23\\) 8059 5000\"\n",
    "countryRE = \"([C][o][u][n][t][r][y][:])\\s*([A-Z][a-z]+,(\\s*[A-Z]*[a-z]+)+)\"\n",
    "#etc\n",
    "\n",
    "for line in codecs.open(fname,\"r\",encoding=\"utf-8\"):\n",
    "    match=re.findall(hashtag_re, line)\n",
    "    if match: print(match)\n",
    "    match=re.findall(postcode_re, line)\n",
    "    if match: print(match)\n",
    "    match=re.findall(phone_re, line)\n",
    "    if match: print(match)\n",
    "    match=re.search(countryRE, line)\n",
    "    if match: print(match)\n",
    "    #etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that was too easy for you, look up the rules for the officially allowable formats of each token type in Wikipedia. Here for example, are the official UK postcode formats:\n",
    "\n",
    "| Format\t    | Coverage\t| Example |\n",
    "| -------:      | :--------:  | :------- |\n",
    "| AA9A 9AA\t| WC postcode area; EC1–EC4, NW1W, SE1P, SW1\t| EC1A 1BB |\n",
    "| A9A 9AA\t| E1, N1, W1\t| W1A 0AX |\n",
    "| A9 9AA\t| B, E, G, L, M, N, S, W\t| M1 1AE |\n",
    "| A99 9AA\t| \" | B33 8TH |\n",
    "| AA9 9AA\t| All other postcodes\t| CR2 6XH  |\n",
    "| AA99 9AA\t| \" | DN55 1PT |\n",
    "\n",
    "The rules for email adddresses are eye-watering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - numeric tokens\n",
    "\n",
    "The zip file [guardian.zip](../corpus/guardian.zip) contains the text extracted from 118 Guardian news stories from the last year about Southampton, Portsmouth and Winchester. (One story per UTF-8 file, but you might like to combine them into a single file for ease of processing.) Starting from the regexp tokenizer example in Fig 2.12 (reproduced below), extend the set of tokens recognised to capture the following types of numeric data.\n",
    "- Numbers: -12  47.2  74,832,101\n",
    "- Time: 09:17pm\n",
    "- Money: £27.8m £8bn\n",
    "- Length: 6ft 48cm\n",
    "- Phone: +44(0)2380594479\n",
    "- Age specification: 13-year-old\n",
    "- Percentage: 14.4%\n",
    "- Temperature: 28C\n",
    "- Ordinals: 48th 22nd 1st\n",
    "\n",
    "You can compare your results with the [list of numeric tokens that I managed to extract](../corpus/guardian-numerics.txt). What’s the biggest financial quantity that appears in these stories? What did it relate to? What are the most common numeric tokens, and why do they appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text='That U.S.A. poster-print costs $12.40...'\n",
    "\n",
    "# The following pattern is reproduced from the textbook figure 2.12.\n",
    "# UNFORTUNATELY, the behaviour of NLTK has changed since version 3.1,\n",
    "# so that capture groups don't work any more and every set\n",
    "# of grouping parentheses () has now to explicitly declare non-capturing semantics with ?:\n",
    "pattern = r'''(?x)\t\t\t# set flag to allow verbose regexps \n",
    "\t (?:[A-Z]\\.)+\t\t\t# abbreviations, e.g. U.S.A. \n",
    "\t | \\w+(?:-\\w+)*\t\t\t# words with optional internal hyphens \n",
    "\t | \\$?\\d+(?:\\.\\d+)?%?\t# currency and percentages, e.g. $12.40, 82% \n",
    "\t | \\.\\.\\.\t\t\t\t# ellipsis \n",
    "\t | [][.,;\"'?():-_`]\t\t# these are separate punctuation tokens; includes ], [ \n",
    "\t '''\n",
    "\n",
    "tokens=nltk.regexp_tokenize(text, pattern)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - lookahead and lookbehind assertions\n",
    "Use the lookaround capabilities to match the following (see the lookaround slide in lecture 3 as a start)\n",
    "- a password with at least 6 characters, containing 2 upper case letters, two digits and two punctuation marks\n",
    "- the above password, but it can't start with AB, ab, 01 or 12 to trap the really obvious abcd... and 1234... passwords\n",
    "- separate out the different components of a camelCase word (e.g. getElementById -> [get, Element, by, Id])\n",
    "-- Hint: identify each location where the previous character is lowercase and the next character is uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 4 - Analysing Lightbulb Jokes\n",
    "There were more than 10k lightbulb jokes told on Twitter in in 2015. Here are a couple of examples of the genre.\n",
    "- Q: How many thriller writers does it take to change a lightbulb? A: Two. One to screw it almost all the way in and another to give a surprising twist at the end.\n",
    "- Do you know how many folk musicians it takes to change a lightbulb? Five. One to change the lightbulb, and four to write songs about how much better the old bulb was.\n",
    "\n",
    "The standard pattern for the opening of a lightbulb joke is _How many X does it take to change a lightbulb?_\n",
    "\n",
    "The file [lightbulbs-2015.txt](../corpus/lightbulbs-2015.txt) contains one lightbulb joke tweet per line.\n",
    "Write a python code that uses regular expressions to isolates the topic X of each of the jokes and use it to produce a summary of the top 100 topics of lightbulb humour.\n",
    "- You should throw away the answers (punchlines) to the joke, just look for the topic\n",
    "- Hint: you will need to use trial and error to deal with variations in language, case and punctuation. You may want to test sets of regular expressions in an editor such as VI to allow you to see the coverage and special cases before you commit them to python code.\n",
    "- This is unmoderated Internet gathered data. Apologies for any inappropriate language that it might contain, or any examples of offensive humour. If you find anything that should be removed, please let me know.\n",
    "\n",
    "The file [lightbulbs-2020.txt](../corpus/lightbulbs-2020.txt) contains the same data for last year. Use the regular expressions you developed to generate a top 100 summary for 2020 and compare the two years. What significant changes in topics have there been between 2015 and 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the file beforehand\n",
    "fname=\"../../corpus/comp3225/lightbulbs-2015.txt\"\n",
    "\n",
    "#make the right regular expression to isolate just the topic\n",
    "pattern=\"what (.*) what\"\n",
    "\n",
    "lineN=0; matchN=0\n",
    "for line in codecs.open(fname,\"r\",encoding='utf-8'):\n",
    "    lineN+=1\n",
    "    match=re.search(pattern, line)\n",
    "    if match:\n",
    "        matchN+=1\n",
    "        print(match.group(1))\n",
    "print(\"The regexp matched %s jokes out of %s\" % (matchN, lineN)) \n",
    "\n",
    "#now go on and do the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Task 5 - citations\n",
    "\n",
    "The file [thesis.txt](https://secure.ecs.soton.ac.uk/notes/comp3225/labs/citations-thesis.txt) contains the text of a PhD thesis. The examiner wants to know how many papers the student has cited, and of those, how many are “recent” (i.e. from the last decade).\n",
    "The bibliographic style used means that citations appear in the text with the basic form (surnames year), and you can see examples of this as they appear in the PDF <img src=\"https://secure.ecs.soton.ac.uk/notes/comp3225/labs/citations-sample.png\" width=\"400\">\n",
    "\n",
    "Write regular expressions to extract the list of citations from the thesis, answering the examiner's questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task 6 - ELIZA\n",
    "\n",
    "ELIZA was an early natural language processing computer program created in 1964 (the year I was born) at the MIT Artificial Intelligence Laboratory and created to demonstrate superficial communication techniques (OMG - it's like we're twins).\n",
    "\n",
    "Eliza simulated conversation by using a \"pattern matching\" and substitution methodology that gave users an illusion of understanding on the part of the program. Famously ELIZA simulated a style of psychotherapist, well-known for simply parroting back at patients what they had just said, and used rules to respond with non-directional questions to user inputs. As such, ELIZA was one of the first chatterbots and one of the first programs capable of attempting the Turing test.\n",
    "\n",
    "You should implement an ELIZA-like conversational program, using substitutions such as those described on page 11 in the text book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
